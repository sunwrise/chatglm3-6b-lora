{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1727c955-c819-4a85-b3c6-9e2edb30bbe1",
   "metadata": {},
   "source": [
    "## 基于广告集数据用lora方法微调chatglm6B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430608d4-144d-4aa3-8b16-7337354ce2aa",
   "metadata": {},
   "source": [
    "### 1. 下载chatglm6B，进行微调前的推理体验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "189e9162-7d36-431f-910e-7db0f6e54751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0a223e10db14ddc9449b9a02a991aae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "这是一款性感的百褶网纱连衣裙，裙下摆采用压褶设计，显得更加立体和修身。整体风格为性感，适合各种场合穿着。\n",
      "\n",
      "裙衣门襟为拉链设计，方便穿脱。套头裙款式，无需脱卸脱卸衣衫，方便快捷。拼接和拉链的设计为裙款式增添了时尚元素。木耳边的设计和抽褶、不规则的裙款式相互呼应，让这款连衣裙更加有层次感。\n",
      "\n",
      "这款连衣裙的材质为网纱，轻盈透气，带有一丝性感的气息。裙长适中，既能展现高挑的身材，又不失优雅。整体设计为百褶裙型，显瘦效果显著，让你在穿着过程中更加自信。\n"
     ]
    }
   ],
   "source": [
    "#测试未微调之前的效果\n",
    "# !python inference_hf.py ../autodl-tmp/model/chatglm3 --prompt \"类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "path =\"../autodl-tmp/model/chatglm3\" #THUDM/chatglm3-6b\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(path, trust_remote_code=True, device='cuda')\n",
    "model = model.eval()\n",
    "response, history = model.chat(tokenizer, \"类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则\", history=[])\n",
    "print(response)\n",
    "\n",
    "# response, history = model.chat(tokenizer, \"晚上睡不着应该怎么办\", history=history)\n",
    "# print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4744129b-d2d2-46d8-ab8c-fe9c5a0a3ff8",
   "metadata": {},
   "source": [
    "### 2. 准备数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3c0d95f-be6a-40b7-b00d-6b502b60bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Union\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def _resolve_path(path: Union[str, Path]) -> Path:\n",
    "    return Path(path).expanduser().resolve()\n",
    "\n",
    "\n",
    "def _mkdir(dir_name: Union[str, Path]):\n",
    "    dir_name = _resolve_path(dir_name)\n",
    "    if not dir_name.is_dir():\n",
    "        dir_name.mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "\n",
    "def convert_adgen(data_dir: Union[str, Path], save_dir: Union[str, Path]):\n",
    "    def _convert(in_file: Path, out_file: Path):\n",
    "        _mkdir(out_file.parent)\n",
    "        with open(in_file, encoding='utf-8') as fin:\n",
    "            # 使用 with 语句打开输出文件 out_file，并将其赋值给变量 fout。指定了以文本模式写入（'wt'）和 UTF-8 编码打开文件。\n",
    "            with open(out_file, 'wt', encoding='utf-8') as fout:\n",
    "                for line in fin:\n",
    "                    # 将读取的每行内容解析为 JSON 格式，存储在变量 dct 中。\n",
    "                    dct = json.loads(line)\n",
    "                    # 根据解析得到的 JSON 数据构造一个新的数据结构 sample，其中包含一个键为 'conversations' 的列表，列表中包含两个字典，分别表示用户和助手的对话内容。\n",
    "                    sample = {'conversations': [{'role': 'user', 'content': dct['content']},\n",
    "                                                {'role': 'assistant', 'content': dct['summary']}]}\n",
    "                    # 将构造好的 sample 对象转换为 JSON 字符串，并写入到输出文件 fout 中。\n",
    "                    # ensure_ascii=False 表示输出的 JSON 字符串中允许包含非 ASCII 字符，+ '\\n' 则是为了在每行结尾添加换行符，使输出文件每行一条数据。\n",
    "                    fout.write(json.dumps(sample, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    # 如果 data_dir 是相对路径 ./data，经过 _resolve_path 处理后，可能会得到 /path/to/current/directory/data 这样的绝对路径。\n",
    "    # 这样做的好处是，无论当前工作目录在哪里，都可以准确地找到数据和保存目录，避免了路径混乱或不确定性带来的问题。\n",
    "    data_dir = _resolve_path(data_dir)\n",
    "    save_dir = _resolve_path(save_dir)\n",
    "\n",
    "    train_file = data_dir / 'train.json'\n",
    "    if train_file.is_file():\n",
    "        out_file = save_dir / train_file.relative_to(data_dir)\n",
    "        _convert(train_file, out_file)\n",
    "\n",
    "    dev_file = data_dir / 'dev.json'\n",
    "    if dev_file.is_file():\n",
    "        out_file = save_dir / dev_file.relative_to(data_dir)\n",
    "        _convert(dev_file, out_file)\n",
    "\n",
    "\n",
    "convert_adgen('data/AdvertiseGen', 'data/AdvertiseGen_fix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cf56a8-1bad-4231-ab57-106f81b8ded8",
   "metadata": {},
   "source": [
    "### 3.微调模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dcf24fb-f261-44a5-8c85-931b98b42ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:01<00:00,  3.99it/s]\n",
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.031217444255383614\n",
      "--> Model\n",
      "\n",
      "--> model has 1.949696M params\n",
      "\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 114599 examples [00:00, 551704.99 examples/s]\n",
      "Setting num_proc from 16 back to 1 for the validation split to disable multiprocessing as it only contains one shard.\n",
      "Generating validation split: 1070 examples [00:00, 129122.34 examples/s]\n",
      "Setting num_proc from 16 back to 1 for the test split to disable multiprocessing as it only contains one shard.\n",
      "Generating test split: 1070 examples [00:00, 125423.54 examples/s]\n",
      "原始列名：['conversations']\n",
      "Map (num_proc=16): 100%|██████| 114599/114599 [00:02<00:00, 41599.13 examples/s]\n",
      "train_dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 114599\n",
      "})\n",
      "原始列名：['conversations']\n",
      "Map (num_proc=16): 100%|████████████| 1070/1070 [00:01<00:00, 945.07 examples/s]\n",
      "val_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "原始列名：['conversations']\n",
      "Map (num_proc=16): 100%|███████████| 1070/1070 [00:01<00:00, 1025.57 examples/s]\n",
      "test_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "--> Sanity check\n",
      "           '[gMASK]': 64790 -> -100\n",
      "               'sop': 64792 -> -100\n",
      "          '<|user|>': 64795 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '\\n': 13 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '类型': 33467 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '版': 55090 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '宽松': 40833 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '风格': 32799 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '性感': 40589 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '图案': 37505 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '线条': 37216 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '阔': 56529 -> -100\n",
      "                 '腿': 56158 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "     '<|assistant|>': 64796 -> -100\n",
      "                  '': 30910 -> 30910\n",
      "                '\\n': 13 -> 13\n",
      "                  '': 30910 -> 30910\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '阔': 56529 -> 56529\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '这': 54551 -> 54551\n",
      "                '两年': 33808 -> 33808\n",
      "                '真的': 32041 -> 32041\n",
      "                 '吸': 55360 -> 55360\n",
      "                 '粉': 55486 -> 55486\n",
      "                '不少': 32138 -> 32138\n",
      "                 '，': 31123 -> 31123\n",
      "                '明星': 32943 -> 32943\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '达': 54880 -> 54880\n",
      "                '人的': 31664 -> 31664\n",
      "                '心头': 46565 -> 46565\n",
      "                 '爱': 54799 -> 54799\n",
      "                 '。': 31155 -> 31155\n",
      "                '毕竟': 33051 -> 33051\n",
      "                 '好': 54591 -> 54591\n",
      "                 '穿': 55432 -> 55432\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '，': 31123 -> 31123\n",
      "                 '谁': 55622 -> 55622\n",
      "                '都能': 32904 -> 32904\n",
      "                 '穿': 55432 -> 55432\n",
      "                 '出': 54557 -> 54557\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '长': 54625 -> 54625\n",
      "                 '2': 30943 -> 30943\n",
      "                 '米': 55055 -> 55055\n",
      "               '的效果': 35590 -> 35590\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '，': 31123 -> 31123\n",
      "               '当然是': 48466 -> 48466\n",
      "                 '遮': 57148 -> 57148\n",
      "                 '肉': 55343 -> 55343\n",
      "                 '小': 54603 -> 54603\n",
      "                '能手': 49355 -> 49355\n",
      "                 '啊': 55674 -> 55674\n",
      "                 '。': 31155 -> 31155\n",
      "                '上身': 51605 -> 51605\n",
      "                 '随': 55119 -> 55119\n",
      "                 '性': 54642 -> 54642\n",
      "                '自然': 31799 -> 31799\n",
      "                 '不': 54535 -> 54535\n",
      "                 '拘': 57036 -> 57036\n",
      "                 '束': 55625 -> 55625\n",
      "                 '，': 31123 -> 31123\n",
      "                '面料': 46839 -> 46839\n",
      "                 '亲': 55113 -> 55113\n",
      "                 '肤': 56089 -> 56089\n",
      "                '舒适': 33894 -> 33894\n",
      "                 '贴': 55778 -> 55778\n",
      "                '身体': 31902 -> 31902\n",
      "                 '验': 55017 -> 55017\n",
      "                 '感': 54706 -> 54706\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '哒': 59230 -> 59230\n",
      "                 '。': 31155 -> 31155\n",
      "                 '系': 54712 -> 54712\n",
      "                 '带': 54882 -> 54882\n",
      "                '部分': 31726 -> 31726\n",
      "                '增加': 31917 -> 31917\n",
      "                '设计': 31735 -> 31735\n",
      "                '看点': 45032 -> 45032\n",
      "                 '，': 31123 -> 31123\n",
      "                 '还': 54656 -> 54656\n",
      "                 '让': 54772 -> 54772\n",
      "                '单品': 46539 -> 46539\n",
      "               '的设计': 34481 -> 34481\n",
      "                 '感': 54706 -> 54706\n",
      "                '更强': 43084 -> 43084\n",
      "                 '。': 31155 -> 31155\n",
      "                '腿部': 46799 -> 46799\n",
      "                '线条': 37216 -> 37216\n",
      "                 '若': 55351 -> 55351\n",
      "                 '隐': 55733 -> 55733\n",
      "                 '若': 55351 -> 55351\n",
      "                 '现': 54600 -> 54600\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                '性感': 40589 -> 40589\n",
      "                 '撩': 58521 -> 58521\n",
      "                 '人': 54533 -> 54533\n",
      "                 '。': 31155 -> 31155\n",
      "                '颜色': 33692 -> 33692\n",
      "                 '敲': 57004 -> 57004\n",
      "                '温柔': 34678 -> 34678\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                 '与': 54619 -> 54619\n",
      "                '裤子': 44722 -> 44722\n",
      "                '本身': 32754 -> 32754\n",
      "                 '所': 54626 -> 54626\n",
      "                '呈现': 33169 -> 33169\n",
      "               '的风格': 48084 -> 48084\n",
      "                '有点': 33149 -> 33149\n",
      "                 '反': 54955 -> 54955\n",
      "                 '差': 55342 -> 55342\n",
      "                 '萌': 56842 -> 56842\n",
      "                 '。': 31155 -> 31155\n",
      "                  '': 2 -> 2\n",
      "/root/miniconda3/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "***** Running training *****\n",
      "  Num examples = 114,599\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3,000\n",
      "  Number of trainable parameters = 1,949,696\n",
      "{'loss': 4.8301, 'grad_norm': 2.2214372158050537, 'learning_rate': 4.9833333333333336e-05, 'epoch': 0.0}\n",
      "{'loss': 4.6, 'grad_norm': 3.1857402324676514, 'learning_rate': 4.966666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 4.4855, 'grad_norm': 2.9991085529327393, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.0}\n",
      "{'loss': 4.1186, 'grad_norm': 3.375319242477417, 'learning_rate': 4.933333333333334e-05, 'epoch': 0.0}\n",
      "{'loss': 4.1127, 'grad_norm': 2.7479407787323, 'learning_rate': 4.9166666666666665e-05, 'epoch': 0.0}\n",
      "{'loss': 3.8646, 'grad_norm': 2.982184648513794, 'learning_rate': 4.9e-05, 'epoch': 0.0}\n",
      "{'loss': 3.841, 'grad_norm': 2.8912603855133057, 'learning_rate': 4.883333333333334e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7449, 'grad_norm': 2.9630815982818604, 'learning_rate': 4.866666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6361, 'grad_norm': 3.243187189102173, 'learning_rate': 4.85e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7195, 'grad_norm': 3.3971781730651855, 'learning_rate': 4.8333333333333334e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6727, 'grad_norm': 3.6045732498168945, 'learning_rate': 4.8166666666666674e-05, 'epoch': 0.0}\n",
      "{'loss': 3.8494, 'grad_norm': 3.916802406311035, 'learning_rate': 4.8e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6156, 'grad_norm': 3.515061616897583, 'learning_rate': 4.7833333333333335e-05, 'epoch': 0.0}\n",
      "{'loss': 3.7324, 'grad_norm': 4.4362664222717285, 'learning_rate': 4.766666666666667e-05, 'epoch': 0.0}\n",
      "{'loss': 3.6838, 'grad_norm': 3.6673688888549805, 'learning_rate': 4.75e-05, 'epoch': 0.01}\n",
      "{'loss': 3.7461, 'grad_norm': 3.946669340133667, 'learning_rate': 4.7333333333333336e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5748, 'grad_norm': 4.1156134605407715, 'learning_rate': 4.716666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.576, 'grad_norm': 4.302922248840332, 'learning_rate': 4.7e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5486, 'grad_norm': 4.812228202819824, 'learning_rate': 4.683333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5762, 'grad_norm': 4.494087219238281, 'learning_rate': 4.666666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.55, 'grad_norm': 4.941982746124268, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6447, 'grad_norm': 4.040960311889648, 'learning_rate': 4.633333333333333e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6117, 'grad_norm': 4.759801864624023, 'learning_rate': 4.6166666666666666e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5125, 'grad_norm': 4.511586666107178, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4773, 'grad_norm': 5.3582963943481445, 'learning_rate': 4.5833333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.602, 'grad_norm': 5.277102470397949, 'learning_rate': 4.566666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5506, 'grad_norm': 5.383239269256592, 'learning_rate': 4.55e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6133, 'grad_norm': 4.538790225982666, 'learning_rate': 4.5333333333333335e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6275, 'grad_norm': 4.757887363433838, 'learning_rate': 4.516666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5395, 'grad_norm': 5.892356872558594, 'learning_rate': 4.5e-05, 'epoch': 0.01}\n",
      "{'loss': 3.467, 'grad_norm': 5.290830135345459, 'learning_rate': 4.483333333333333e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6027, 'grad_norm': 5.80058479309082, 'learning_rate': 4.466666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4174, 'grad_norm': 5.21953010559082, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4947, 'grad_norm': 5.276256561279297, 'learning_rate': 4.433333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5193, 'grad_norm': 5.4948835372924805, 'learning_rate': 4.4166666666666665e-05, 'epoch': 0.01}\n",
      "{'loss': 3.575, 'grad_norm': 5.239449977874756, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.01}\n",
      "{'loss': 3.3607, 'grad_norm': 4.87606143951416, 'learning_rate': 4.383333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5285, 'grad_norm': 5.1886749267578125, 'learning_rate': 4.3666666666666666e-05, 'epoch': 0.01}\n",
      "{'loss': 3.5238, 'grad_norm': 5.283796787261963, 'learning_rate': 4.35e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4748, 'grad_norm': 5.6479716300964355, 'learning_rate': 4.3333333333333334e-05, 'epoch': 0.01}\n",
      "{'loss': 3.6914, 'grad_norm': 5.541319370269775, 'learning_rate': 4.316666666666667e-05, 'epoch': 0.01}\n",
      "{'loss': 3.4975, 'grad_norm': 5.039565563201904, 'learning_rate': 4.3e-05, 'epoch': 0.01}\n",
      "{'loss': 3.627, 'grad_norm': 5.594403266906738, 'learning_rate': 4.2833333333333335e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4166, 'grad_norm': 6.597824573516846, 'learning_rate': 4.266666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4135, 'grad_norm': 6.051794528961182, 'learning_rate': 4.25e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4229, 'grad_norm': 5.596541881561279, 'learning_rate': 4.233333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5328, 'grad_norm': 5.825827598571777, 'learning_rate': 4.216666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.448, 'grad_norm': 7.070689678192139, 'learning_rate': 4.2e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4572, 'grad_norm': 5.806086540222168, 'learning_rate': 4.183333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5625, 'grad_norm': 5.930208206176758, 'learning_rate': 4.166666666666667e-05, 'epoch': 0.02}\n",
      " 17%|██████▋                                 | 500/3000 [04:44<26:48,  1.55it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.37s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:16<00:05,  5.21s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:29<00:00,  8.30s/it]\u001b[ABuilding prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 0.468 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "                                                                                \n",
      "\u001b[A{'eval_rouge-1': 31.606784, 'eval_rouge-2': 7.0031099999999995, 'eval_rouge-l': 23.266266, 'eval_bleu-4': 0.030134442975491255, 'eval_runtime': 45.8001, 'eval_samples_per_second': 1.092, 'eval_steps_per_second': 0.087, 'epoch': 0.02}\n",
      " 17%|██████▋                                 | 500/3000 [05:30<26:48,  1.55it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:30<00:00,  8.30s/it]\u001b[A\n",
      "{'loss': 3.3203, 'grad_norm': 5.735914707183838, 'learning_rate': 4.15e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5451, 'grad_norm': 6.6801910400390625, 'learning_rate': 4.133333333333333e-05, 'epoch': 0.02}\n",
      "{'loss': 3.584, 'grad_norm': 6.018150806427002, 'learning_rate': 4.116666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4834, 'grad_norm': 5.432873725891113, 'learning_rate': 4.1e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5236, 'grad_norm': 5.431870937347412, 'learning_rate': 4.0833333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 3.6469, 'grad_norm': 5.792478561401367, 'learning_rate': 4.066666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4947, 'grad_norm': 5.876513957977295, 'learning_rate': 4.05e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3721, 'grad_norm': 5.621215343475342, 'learning_rate': 4.0333333333333336e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4262, 'grad_norm': 6.276374340057373, 'learning_rate': 4.016666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.49, 'grad_norm': 6.533111095428467, 'learning_rate': 4e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4373, 'grad_norm': 6.199402809143066, 'learning_rate': 3.983333333333333e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4559, 'grad_norm': 6.770179271697998, 'learning_rate': 3.966666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4479, 'grad_norm': 6.090966701507568, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.02}\n",
      "{'loss': 3.458, 'grad_norm': 6.237969398498535, 'learning_rate': 3.933333333333333e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5352, 'grad_norm': 5.912956237792969, 'learning_rate': 3.9166666666666665e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4859, 'grad_norm': 6.357933521270752, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5422, 'grad_norm': 6.175895690917969, 'learning_rate': 3.883333333333333e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3014, 'grad_norm': 7.067182540893555, 'learning_rate': 3.866666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3977, 'grad_norm': 6.6494598388671875, 'learning_rate': 3.85e-05, 'epoch': 0.02}\n",
      "{'loss': 3.3543, 'grad_norm': 6.3929901123046875, 'learning_rate': 3.8333333333333334e-05, 'epoch': 0.02}\n",
      "{'loss': 3.4996, 'grad_norm': 6.987468242645264, 'learning_rate': 3.816666666666667e-05, 'epoch': 0.02}\n",
      "{'loss': 3.5273, 'grad_norm': 6.819683074951172, 'learning_rate': 3.8e-05, 'epoch': 0.03}\n",
      "{'loss': 3.2471, 'grad_norm': 6.959506988525391, 'learning_rate': 3.7833333333333336e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5725, 'grad_norm': 5.936046123504639, 'learning_rate': 3.766666666666667e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3979, 'grad_norm': 6.488315582275391, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.03}\n",
      "{'loss': 3.476, 'grad_norm': 6.196877479553223, 'learning_rate': 3.733333333333334e-05, 'epoch': 0.03}\n",
      "{'loss': 3.6213, 'grad_norm': 6.445000171661377, 'learning_rate': 3.7166666666666664e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4738, 'grad_norm': 6.375843524932861, 'learning_rate': 3.7e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3242, 'grad_norm': 6.552029132843018, 'learning_rate': 3.683333333333334e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5492, 'grad_norm': 6.906155109405518, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.03}\n",
      "{'loss': 3.2904, 'grad_norm': 6.576301574707031, 'learning_rate': 3.65e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3584, 'grad_norm': 6.528741359710693, 'learning_rate': 3.633333333333333e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4646, 'grad_norm': 7.242380142211914, 'learning_rate': 3.6166666666666674e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4014, 'grad_norm': 6.395750522613525, 'learning_rate': 3.6e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5076, 'grad_norm': 6.276432037353516, 'learning_rate': 3.5833333333333335e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5332, 'grad_norm': 6.292588233947754, 'learning_rate': 3.566666666666667e-05, 'epoch': 0.03}\n",
      "{'loss': 3.2924, 'grad_norm': 7.3074259757995605, 'learning_rate': 3.55e-05, 'epoch': 0.03}\n",
      "{'loss': 3.493, 'grad_norm': 6.7980875968933105, 'learning_rate': 3.5333333333333336e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4525, 'grad_norm': 7.502225399017334, 'learning_rate': 3.516666666666667e-05, 'epoch': 0.03}\n",
      "{'loss': 3.265, 'grad_norm': 8.006996154785156, 'learning_rate': 3.5e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4594, 'grad_norm': 7.869502544403076, 'learning_rate': 3.483333333333334e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4201, 'grad_norm': 7.056487560272217, 'learning_rate': 3.466666666666667e-05, 'epoch': 0.03}\n",
      "{'loss': 3.46, 'grad_norm': 7.707385540008545, 'learning_rate': 3.45e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5676, 'grad_norm': 7.44580602645874, 'learning_rate': 3.433333333333333e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3613, 'grad_norm': 6.511038780212402, 'learning_rate': 3.4166666666666666e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4393, 'grad_norm': 7.991611480712891, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.03}\n",
      "{'loss': 3.5348, 'grad_norm': 6.045433521270752, 'learning_rate': 3.3833333333333334e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3234, 'grad_norm': 7.127168655395508, 'learning_rate': 3.366666666666667e-05, 'epoch': 0.03}\n",
      "{'loss': 3.4605, 'grad_norm': 7.508118152618408, 'learning_rate': 3.35e-05, 'epoch': 0.03}\n",
      "{'loss': 3.3971, 'grad_norm': 8.033147811889648, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.03}\n",
      " 33%|█████████████                          | 1000/3000 [10:10<19:31,  1.71it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.65s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:05<00:01,  1.70s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.049738, 'eval_rouge-2': 6.989586, 'eval_rouge-l': 25.525684000000002, 'eval_bleu-4': 0.033553726894836336, 'eval_runtime': 21.8202, 'eval_samples_per_second': 2.291, 'eval_steps_per_second': 0.183, 'epoch': 0.03}\n",
      " 33%|█████████████                          | 1000/3000 [10:32<19:31,  1.71it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:07<00:00,  1.86s/it]\u001b[A\n",
      "{'loss': 3.4506, 'grad_norm': 6.955600261688232, 'learning_rate': 3.316666666666667e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4582, 'grad_norm': 7.570169925689697, 'learning_rate': 3.3e-05, 'epoch': 0.04}\n",
      "{'loss': 3.6506, 'grad_norm': 8.408422470092773, 'learning_rate': 3.283333333333333e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4018, 'grad_norm': 6.5387420654296875, 'learning_rate': 3.266666666666667e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3896, 'grad_norm': 8.688844680786133, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3555, 'grad_norm': 7.83032751083374, 'learning_rate': 3.233333333333333e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3898, 'grad_norm': 7.358563423156738, 'learning_rate': 3.2166666666666665e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4596, 'grad_norm': 7.265598773956299, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5314, 'grad_norm': 7.240787982940674, 'learning_rate': 3.183333333333334e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4666, 'grad_norm': 6.595025062561035, 'learning_rate': 3.1666666666666666e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3424, 'grad_norm': 6.900148868560791, 'learning_rate': 3.15e-05, 'epoch': 0.04}\n",
      "{'loss': 3.5287, 'grad_norm': 8.09432601928711, 'learning_rate': 3.1333333333333334e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4342, 'grad_norm': 7.443538665771484, 'learning_rate': 3.116666666666667e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3594, 'grad_norm': 8.136141777038574, 'learning_rate': 3.1e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3205, 'grad_norm': 7.6373443603515625, 'learning_rate': 3.0833333333333335e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3639, 'grad_norm': 7.272091388702393, 'learning_rate': 3.066666666666667e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4531, 'grad_norm': 6.7857770919799805, 'learning_rate': 3.05e-05, 'epoch': 0.04}\n",
      "{'loss': 3.475, 'grad_norm': 6.501801490783691, 'learning_rate': 3.0333333333333337e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3545, 'grad_norm': 6.734158515930176, 'learning_rate': 3.016666666666667e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4104, 'grad_norm': 6.493255615234375, 'learning_rate': 3e-05, 'epoch': 0.04}\n",
      "{'loss': 3.243, 'grad_norm': 6.641349792480469, 'learning_rate': 2.9833333333333335e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3383, 'grad_norm': 7.4883270263671875, 'learning_rate': 2.9666666666666672e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3828, 'grad_norm': 7.593662738800049, 'learning_rate': 2.95e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3773, 'grad_norm': 8.336164474487305, 'learning_rate': 2.9333333333333336e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4439, 'grad_norm': 6.837422847747803, 'learning_rate': 2.916666666666667e-05, 'epoch': 0.04}\n",
      "{'loss': 3.2807, 'grad_norm': 7.864181041717529, 'learning_rate': 2.9e-05, 'epoch': 0.04}\n",
      "{'loss': 3.4592, 'grad_norm': 7.318397045135498, 'learning_rate': 2.8833333333333334e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3334, 'grad_norm': 7.368082523345947, 'learning_rate': 2.8666666666666668e-05, 'epoch': 0.04}\n",
      "{'loss': 3.3885, 'grad_norm': 7.0140581130981445, 'learning_rate': 2.8499999999999998e-05, 'epoch': 0.05}\n",
      "{'loss': 3.483, 'grad_norm': 7.546838760375977, 'learning_rate': 2.8333333333333335e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4592, 'grad_norm': 7.000410079956055, 'learning_rate': 2.816666666666667e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4516, 'grad_norm': 6.778518199920654, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3996, 'grad_norm': 10.267254829406738, 'learning_rate': 2.7833333333333333e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3018, 'grad_norm': 7.593081474304199, 'learning_rate': 2.7666666666666667e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3479, 'grad_norm': 7.628702163696289, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.05}\n",
      "{'loss': 3.298, 'grad_norm': 8.025627136230469, 'learning_rate': 2.733333333333333e-05, 'epoch': 0.05}\n",
      "{'loss': 3.516, 'grad_norm': 7.197702407836914, 'learning_rate': 2.716666666666667e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3832, 'grad_norm': 7.310572147369385, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3588, 'grad_norm': 7.2667670249938965, 'learning_rate': 2.6833333333333333e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4215, 'grad_norm': 6.730660438537598, 'learning_rate': 2.6666666666666667e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3471, 'grad_norm': 7.593268394470215, 'learning_rate': 2.6500000000000004e-05, 'epoch': 0.05}\n",
      "{'loss': 3.2643, 'grad_norm': 7.910378456115723, 'learning_rate': 2.633333333333333e-05, 'epoch': 0.05}\n",
      "{'loss': 3.383, 'grad_norm': 7.844292640686035, 'learning_rate': 2.6166666666666668e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3627, 'grad_norm': 7.317197322845459, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.05}\n",
      "{'loss': 3.2699, 'grad_norm': 7.040022850036621, 'learning_rate': 2.5833333333333336e-05, 'epoch': 0.05}\n",
      "{'loss': 3.391, 'grad_norm': 7.31523323059082, 'learning_rate': 2.5666666666666666e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4375, 'grad_norm': 9.277195930480957, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.05}\n",
      "{'loss': 3.2953, 'grad_norm': 6.880609512329102, 'learning_rate': 2.5333333333333337e-05, 'epoch': 0.05}\n",
      "{'loss': 3.443, 'grad_norm': 7.467473983764648, 'learning_rate': 2.5166666666666667e-05, 'epoch': 0.05}\n",
      "{'loss': 3.458, 'grad_norm': 6.91604471206665, 'learning_rate': 2.5e-05, 'epoch': 0.05}\n",
      " 50%|███████████████████▌                   | 1500/3000 [15:12<12:29,  2.00it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:03<00:03,  1.60s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:05<00:01,  1.77s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.100108, 'eval_rouge-2': 6.929227999999999, 'eval_rouge-l': 25.138026, 'eval_bleu-4': 0.03319334408919623, 'eval_runtime': 20.9613, 'eval_samples_per_second': 2.385, 'eval_steps_per_second': 0.191, 'epoch': 0.05}\n",
      " 50%|███████████████████▌                   | 1500/3000 [15:33<12:29,  2.00it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:07<00:00,  1.73s/it]\u001b[A\n",
      "{'loss': 3.3455, 'grad_norm': 7.113892078399658, 'learning_rate': 2.4833333333333335e-05, 'epoch': 0.05}\n",
      "{'loss': 3.3891, 'grad_norm': 8.306452751159668, 'learning_rate': 2.466666666666667e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4377, 'grad_norm': 8.214991569519043, 'learning_rate': 2.45e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4027, 'grad_norm': 6.972714900970459, 'learning_rate': 2.4333333333333336e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4947, 'grad_norm': 7.4231157302856445, 'learning_rate': 2.4166666666666667e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4076, 'grad_norm': 8.655794143676758, 'learning_rate': 2.4e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4688, 'grad_norm': 8.09442138671875, 'learning_rate': 2.3833333333333334e-05, 'epoch': 0.05}\n",
      "{'loss': 3.4367, 'grad_norm': 7.668041229248047, 'learning_rate': 2.3666666666666668e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5074, 'grad_norm': 9.272250175476074, 'learning_rate': 2.35e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3893, 'grad_norm': 7.2000908851623535, 'learning_rate': 2.3333333333333336e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3674, 'grad_norm': 8.11721420288086, 'learning_rate': 2.3166666666666666e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3701, 'grad_norm': 8.478846549987793, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4732, 'grad_norm': 7.402384281158447, 'learning_rate': 2.2833333333333334e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3217, 'grad_norm': 8.129029273986816, 'learning_rate': 2.2666666666666668e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3693, 'grad_norm': 7.533752918243408, 'learning_rate': 2.25e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3016, 'grad_norm': 7.091366767883301, 'learning_rate': 2.2333333333333335e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4797, 'grad_norm': 8.754822731018066, 'learning_rate': 2.216666666666667e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3738, 'grad_norm': 7.345758438110352, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3732, 'grad_norm': 7.483043670654297, 'learning_rate': 2.1833333333333333e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5195, 'grad_norm': 7.113464832305908, 'learning_rate': 2.1666666666666667e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4627, 'grad_norm': 7.422109603881836, 'learning_rate': 2.15e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5023, 'grad_norm': 7.598784923553467, 'learning_rate': 2.1333333333333335e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4021, 'grad_norm': 7.547049045562744, 'learning_rate': 2.116666666666667e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3979, 'grad_norm': 7.6248602867126465, 'learning_rate': 2.1e-05, 'epoch': 0.06}\n",
      "{'loss': 3.467, 'grad_norm': 7.6120285987854, 'learning_rate': 2.0833333333333336e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4434, 'grad_norm': 7.892521858215332, 'learning_rate': 2.0666666666666666e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3598, 'grad_norm': 8.45290756225586, 'learning_rate': 2.05e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3574, 'grad_norm': 8.214702606201172, 'learning_rate': 2.0333333333333334e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3922, 'grad_norm': 8.284760475158691, 'learning_rate': 2.0166666666666668e-05, 'epoch': 0.06}\n",
      "{'loss': 3.34, 'grad_norm': 7.976284503936768, 'learning_rate': 2e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3795, 'grad_norm': 9.127802848815918, 'learning_rate': 1.9833333333333335e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3432, 'grad_norm': 7.857100486755371, 'learning_rate': 1.9666666666666666e-05, 'epoch': 0.06}\n",
      "{'loss': 3.5787, 'grad_norm': 7.975458145141602, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3459, 'grad_norm': 8.860365867614746, 'learning_rate': 1.9333333333333333e-05, 'epoch': 0.06}\n",
      "{'loss': 3.4939, 'grad_norm': 9.268357276916504, 'learning_rate': 1.9166666666666667e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3834, 'grad_norm': 7.6025848388671875, 'learning_rate': 1.9e-05, 'epoch': 0.06}\n",
      "{'loss': 3.3193, 'grad_norm': 8.276835441589355, 'learning_rate': 1.8833333333333335e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3096, 'grad_norm': 7.575780868530273, 'learning_rate': 1.866666666666667e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4043, 'grad_norm': 7.49144983291626, 'learning_rate': 1.85e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3727, 'grad_norm': 7.9188761711120605, 'learning_rate': 1.8333333333333333e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3896, 'grad_norm': 8.22642707824707, 'learning_rate': 1.8166666666666667e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4787, 'grad_norm': 7.500240802764893, 'learning_rate': 1.8e-05, 'epoch': 0.07}\n",
      "{'loss': 3.2818, 'grad_norm': 7.981930732727051, 'learning_rate': 1.7833333333333334e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5006, 'grad_norm': 7.9628376960754395, 'learning_rate': 1.7666666666666668e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3631, 'grad_norm': 7.266645908355713, 'learning_rate': 1.75e-05, 'epoch': 0.07}\n",
      "{'loss': 3.2846, 'grad_norm': 8.779573440551758, 'learning_rate': 1.7333333333333336e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3717, 'grad_norm': 7.7683892250061035, 'learning_rate': 1.7166666666666666e-05, 'epoch': 0.07}\n",
      "{'loss': 3.2402, 'grad_norm': 7.766859531402588, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4197, 'grad_norm': 7.274171352386475, 'learning_rate': 1.6833333333333334e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4689, 'grad_norm': 8.072042465209961, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.07}\n",
      " 67%|██████████████████████████             | 2000/3000 [20:13<09:06,  1.83it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:02<00:02,  1.23s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:14<00:05,  5.90s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 31.953672, 'eval_rouge-2': 6.994807999999999, 'eval_rouge-l': 22.676715999999995, 'eval_bleu-4': 0.03386190666367916, 'eval_runtime': 31.3647, 'eval_samples_per_second': 1.594, 'eval_steps_per_second': 0.128, 'epoch': 0.07}\n",
      " 67%|██████████████████████████             | 2000/3000 [20:44<09:06,  1.83it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:16<00:00,  4.34s/it]\u001b[A\n",
      "                                                                                \u001b[ASaving model checkpoint to ./output/checkpoint-2000\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../autodl-tmp/model/chatglm3 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "{'loss': 3.3902, 'grad_norm': 8.904178619384766, 'learning_rate': 1.65e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4963, 'grad_norm': 7.744819164276123, 'learning_rate': 1.6333333333333335e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5561, 'grad_norm': 8.894948959350586, 'learning_rate': 1.6166666666666665e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4924, 'grad_norm': 8.574150085449219, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3707, 'grad_norm': 8.47715950012207, 'learning_rate': 1.5833333333333333e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3279, 'grad_norm': 7.882842540740967, 'learning_rate': 1.5666666666666667e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4424, 'grad_norm': 8.379859924316406, 'learning_rate': 1.55e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4152, 'grad_norm': 8.374821662902832, 'learning_rate': 1.5333333333333334e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4404, 'grad_norm': 7.543618202209473, 'learning_rate': 1.5166666666666668e-05, 'epoch': 0.07}\n",
      "{'loss': 3.359, 'grad_norm': 7.830572128295898, 'learning_rate': 1.5e-05, 'epoch': 0.07}\n",
      "{'loss': 3.2932, 'grad_norm': 7.819767475128174, 'learning_rate': 1.4833333333333336e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5812, 'grad_norm': 8.284809112548828, 'learning_rate': 1.4666666666666668e-05, 'epoch': 0.07}\n",
      "{'loss': 3.2543, 'grad_norm': 7.870508193969727, 'learning_rate': 1.45e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3627, 'grad_norm': 8.36943531036377, 'learning_rate': 1.4333333333333334e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4002, 'grad_norm': 7.5029730796813965, 'learning_rate': 1.4166666666666668e-05, 'epoch': 0.08}\n",
      "{'loss': 3.5152, 'grad_norm': 8.265189170837402, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3977, 'grad_norm': 7.024610996246338, 'learning_rate': 1.3833333333333334e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4127, 'grad_norm': 7.955397605895996, 'learning_rate': 1.3666666666666666e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3533, 'grad_norm': 7.778101444244385, 'learning_rate': 1.3500000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 3.432, 'grad_norm': 7.651393413543701, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4506, 'grad_norm': 7.023166656494141, 'learning_rate': 1.3166666666666665e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4195, 'grad_norm': 7.940942764282227, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4195, 'grad_norm': 8.271703720092773, 'learning_rate': 1.2833333333333333e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3715, 'grad_norm': 8.468618392944336, 'learning_rate': 1.2666666666666668e-05, 'epoch': 0.08}\n",
      "{'loss': 3.2398, 'grad_norm': 8.478156089782715, 'learning_rate': 1.25e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3646, 'grad_norm': 8.044381141662598, 'learning_rate': 1.2333333333333334e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4295, 'grad_norm': 8.834288597106934, 'learning_rate': 1.2166666666666668e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4668, 'grad_norm': 7.869452953338623, 'learning_rate': 1.2e-05, 'epoch': 0.08}\n",
      "{'loss': 3.2914, 'grad_norm': 8.419678688049316, 'learning_rate': 1.1833333333333334e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3602, 'grad_norm': 8.582236289978027, 'learning_rate': 1.1666666666666668e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3203, 'grad_norm': 8.556329727172852, 'learning_rate': 1.1500000000000002e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3311, 'grad_norm': 8.401065826416016, 'learning_rate': 1.1333333333333334e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3695, 'grad_norm': 9.059467315673828, 'learning_rate': 1.1166666666666668e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3613, 'grad_norm': 7.902379989624023, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 3.2676, 'grad_norm': 8.787368774414062, 'learning_rate': 1.0833333333333334e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3854, 'grad_norm': 8.237810134887695, 'learning_rate': 1.0666666666666667e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3518, 'grad_norm': 7.9809651374816895, 'learning_rate': 1.05e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4936, 'grad_norm': 8.593108177185059, 'learning_rate': 1.0333333333333333e-05, 'epoch': 0.08}\n",
      "{'loss': 3.2373, 'grad_norm': 8.585312843322754, 'learning_rate': 1.0166666666666667e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4574, 'grad_norm': 8.036229133605957, 'learning_rate': 1e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4533, 'grad_norm': 8.17334270477295, 'learning_rate': 9.833333333333333e-06, 'epoch': 0.08}\n",
      "{'loss': 3.2795, 'grad_norm': 8.243270874023438, 'learning_rate': 9.666666666666667e-06, 'epoch': 0.08}\n",
      "{'loss': 3.3709, 'grad_norm': 7.433795928955078, 'learning_rate': 9.5e-06, 'epoch': 0.08}\n",
      "{'loss': 3.3797, 'grad_norm': 8.201465606689453, 'learning_rate': 9.333333333333334e-06, 'epoch': 0.09}\n",
      "{'loss': 3.2771, 'grad_norm': 7.9859538078308105, 'learning_rate': 9.166666666666666e-06, 'epoch': 0.09}\n",
      "{'loss': 3.3139, 'grad_norm': 7.859819412231445, 'learning_rate': 9e-06, 'epoch': 0.09}\n",
      "{'loss': 3.2594, 'grad_norm': 8.828451156616211, 'learning_rate': 8.833333333333334e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4365, 'grad_norm': 7.582235813140869, 'learning_rate': 8.666666666666668e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4736, 'grad_norm': 7.970200061798096, 'learning_rate': 8.500000000000002e-06, 'epoch': 0.09}\n",
      "{'loss': 3.393, 'grad_norm': 9.692683219909668, 'learning_rate': 8.333333333333334e-06, 'epoch': 0.09}\n",
      " 83%|████████████████████████████████▌      | 2500/3000 [25:24<04:35,  1.81it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.35s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:14<00:04,  4.52s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.654104, 'eval_rouge-2': 7.085433999999999, 'eval_rouge-l': 25.217208, 'eval_bleu-4': 0.03440877987131372, 'eval_runtime': 31.0769, 'eval_samples_per_second': 1.609, 'eval_steps_per_second': 0.129, 'epoch': 0.09}\n",
      " 83%|████████████████████████████████▌      | 2500/3000 [25:55<04:35,  1.81it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:16<00:00,  3.45s/it]\u001b[A\n",
      "{'loss': 3.3008, 'grad_norm': 8.584990501403809, 'learning_rate': 8.166666666666668e-06, 'epoch': 0.09}\n",
      "{'loss': 3.3432, 'grad_norm': 10.713668823242188, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.09}\n",
      "{'loss': 3.2502, 'grad_norm': 8.21001148223877, 'learning_rate': 7.833333333333333e-06, 'epoch': 0.09}\n",
      "{'loss': 3.3916, 'grad_norm': 8.341980934143066, 'learning_rate': 7.666666666666667e-06, 'epoch': 0.09}\n",
      "{'loss': 3.3877, 'grad_norm': 7.968461990356445, 'learning_rate': 7.5e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4018, 'grad_norm': 8.61766242980957, 'learning_rate': 7.333333333333334e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4744, 'grad_norm': 8.14546012878418, 'learning_rate': 7.166666666666667e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4838, 'grad_norm': 8.721335411071777, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.09}\n",
      "{'loss': 3.3781, 'grad_norm': 8.634202003479004, 'learning_rate': 6.833333333333333e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4805, 'grad_norm': 8.816553115844727, 'learning_rate': 6.666666666666667e-06, 'epoch': 0.09}\n",
      "{'loss': 3.3602, 'grad_norm': 8.08553409576416, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4238, 'grad_norm': 7.778759956359863, 'learning_rate': 6.333333333333334e-06, 'epoch': 0.09}\n",
      "{'loss': 3.5254, 'grad_norm': 7.819869518280029, 'learning_rate': 6.166666666666667e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4488, 'grad_norm': 8.688854217529297, 'learning_rate': 6e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4111, 'grad_norm': 8.357250213623047, 'learning_rate': 5.833333333333334e-06, 'epoch': 0.09}\n",
      "{'loss': 3.3523, 'grad_norm': 8.052210807800293, 'learning_rate': 5.666666666666667e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4072, 'grad_norm': 8.787894248962402, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.09}\n",
      "{'loss': 3.2713, 'grad_norm': 7.731696605682373, 'learning_rate': 5.333333333333334e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4686, 'grad_norm': 8.8095703125, 'learning_rate': 5.166666666666667e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4473, 'grad_norm': 9.036767959594727, 'learning_rate': 5e-06, 'epoch': 0.09}\n",
      "{'loss': 3.4285, 'grad_norm': 8.462820053100586, 'learning_rate': 4.833333333333333e-06, 'epoch': 0.09}\n",
      "{'loss': 3.2504, 'grad_norm': 7.8605055809021, 'learning_rate': 4.666666666666667e-06, 'epoch': 0.09}\n",
      "{'loss': 3.3805, 'grad_norm': 8.16367244720459, 'learning_rate': 4.5e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3895, 'grad_norm': 8.226597785949707, 'learning_rate': 4.333333333333334e-06, 'epoch': 0.1}\n",
      "{'loss': 3.4553, 'grad_norm': 9.144444465637207, 'learning_rate': 4.166666666666667e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3982, 'grad_norm': 8.394062042236328, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3504, 'grad_norm': 8.561147689819336, 'learning_rate': 3.833333333333334e-06, 'epoch': 0.1}\n",
      "{'loss': 3.2598, 'grad_norm': 8.764328956604004, 'learning_rate': 3.666666666666667e-06, 'epoch': 0.1}\n",
      "{'loss': 3.276, 'grad_norm': 8.294584274291992, 'learning_rate': 3.5000000000000004e-06, 'epoch': 0.1}\n",
      "{'loss': 3.241, 'grad_norm': 7.809208393096924, 'learning_rate': 3.3333333333333333e-06, 'epoch': 0.1}\n",
      "{'loss': 3.4463, 'grad_norm': 8.0474271774292, 'learning_rate': 3.166666666666667e-06, 'epoch': 0.1}\n",
      "{'loss': 3.374, 'grad_norm': 8.135013580322266, 'learning_rate': 3e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3859, 'grad_norm': 8.189294815063477, 'learning_rate': 2.8333333333333335e-06, 'epoch': 0.1}\n",
      "{'loss': 3.4418, 'grad_norm': 9.244010925292969, 'learning_rate': 2.666666666666667e-06, 'epoch': 0.1}\n",
      "{'loss': 3.4041, 'grad_norm': 8.786652565002441, 'learning_rate': 2.5e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3377, 'grad_norm': 8.570768356323242, 'learning_rate': 2.3333333333333336e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3766, 'grad_norm': 8.683405876159668, 'learning_rate': 2.166666666666667e-06, 'epoch': 0.1}\n",
      "{'loss': 3.5098, 'grad_norm': 9.368451118469238, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3061, 'grad_norm': 8.311287879943848, 'learning_rate': 1.8333333333333335e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3297, 'grad_norm': 9.10001277923584, 'learning_rate': 1.6666666666666667e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3055, 'grad_norm': 8.056614875793457, 'learning_rate': 1.5e-06, 'epoch': 0.1}\n",
      "{'loss': 3.2479, 'grad_norm': 7.485116958618164, 'learning_rate': 1.3333333333333334e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3592, 'grad_norm': 9.076703071594238, 'learning_rate': 1.1666666666666668e-06, 'epoch': 0.1}\n",
      "{'loss': 3.2559, 'grad_norm': 8.60040283203125, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.1}\n",
      "{'loss': 3.3771, 'grad_norm': 8.239639282226562, 'learning_rate': 8.333333333333333e-07, 'epoch': 0.1}\n",
      "{'loss': 3.2109, 'grad_norm': 9.15317153930664, 'learning_rate': 6.666666666666667e-07, 'epoch': 0.1}\n",
      "{'loss': 3.45, 'grad_norm': 9.075699806213379, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.1}\n",
      "{'loss': 3.4309, 'grad_norm': 9.138029098510742, 'learning_rate': 3.3333333333333335e-07, 'epoch': 0.1}\n",
      "{'loss': 3.4725, 'grad_norm': 8.270451545715332, 'learning_rate': 1.6666666666666668e-07, 'epoch': 0.1}\n",
      "{'loss': 3.3658, 'grad_norm': 7.887620449066162, 'learning_rate': 0.0, 'epoch': 0.1}\n",
      "100%|███████████████████████████████████████| 3000/3000 [30:35<00:00,  1.88it/s]***** Running Evaluation *****\n",
      "  Num examples = 50\n",
      "  Batch size = 16\n",
      "\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:12<00:12,  6.30s/it]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:25<00:08,  8.85s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_rouge-1': 32.24337, 'eval_rouge-2': 6.982790000000001, 'eval_rouge-l': 23.500570000000003, 'eval_bleu-4': 0.03254721463131392, 'eval_runtime': 52.1392, 'eval_samples_per_second': 0.959, 'eval_steps_per_second': 0.077, 'epoch': 0.1}\n",
      "100%|███████████████████████████████████████| 3000/3000 [31:27<00:00,  1.88it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:37<00:00, 10.29s/it]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1887.2863, 'train_samples_per_second': 6.358, 'train_steps_per_second': 1.59, 'train_loss': 3.4463541666666666, 'epoch': 0.1}\n",
      "100%|███████████████████████████████████████| 3000/3000 [31:27<00:00,  1.59it/s]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1070\n",
      "  Batch size = 16\n",
      "100%|███████████████████████████████████████████| 67/67 [09:17<00:00,  8.31s/it]\n"
     ]
    }
   ],
   "source": [
    "!python finetune_hf.py  data/AdvertiseGen_fix  ../autodl-tmp/model/chatglm3  lora.yaml\n",
    "\n",
    "# 从保存点进行微调\n",
    "# 如果按照上述方式进行训练，每次微调都会从头开始，如果你想从训练一半的模型开始微调，你可以加入第四个参数，这个参数有两种传入方式:\n",
    "\n",
    "# yes, 自动从最后一个保存的 Checkpoint开始训练\n",
    "# XX, 断点号数字 例 600 则从序号600 Checkpoint开始训练\n",
    "# 例如，这就是一个从最后一个保存点继续微调的示例代码\n",
    "\n",
    "# cd finetune_demo\n",
    "# python finetune_hf.py  data/AdvertiseGen/  THUDM/chatglm3-6b  configs/lora.yaml yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06c5e540-8511-4460-a016-2880f72d6b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:01<00:00,  3.95it/s]\n",
      "trainable params: 1,949,696 || all params: 6,245,533,696 || trainable%: 0.031217444255383614\n",
      "--> Model\n",
      "\n",
      "--> model has 1.949696M params\n",
      "\n",
      "原始列名：['conversations']\n",
      "train_dataset: Dataset({\n",
      "    features: ['input_ids', 'labels'],\n",
      "    num_rows: 114599\n",
      "})\n",
      "原始列名：['conversations']\n",
      "val_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "原始列名：['conversations']\n",
      "test_dataset: Dataset({\n",
      "    features: ['input_ids', 'output_ids'],\n",
      "    num_rows: 1070\n",
      "})\n",
      "--> Sanity check\n",
      "           '[gMASK]': 64790 -> -100\n",
      "               'sop': 64792 -> -100\n",
      "          '<|user|>': 64795 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '\\n': 13 -> -100\n",
      "                  '': 30910 -> -100\n",
      "                '类型': 33467 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '版': 55090 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '宽松': 40833 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '风格': 32799 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '性感': 40589 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                '图案': 37505 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                '线条': 37216 -> -100\n",
      "                 '*': 30998 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "                 '型': 54888 -> -100\n",
      "                 '#': 31010 -> -100\n",
      "                 '阔': 56529 -> -100\n",
      "                 '腿': 56158 -> -100\n",
      "                 '裤': 56532 -> -100\n",
      "     '<|assistant|>': 64796 -> -100\n",
      "                  '': 30910 -> 30910\n",
      "                '\\n': 13 -> 13\n",
      "                  '': 30910 -> 30910\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '阔': 56529 -> 56529\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '这': 54551 -> 54551\n",
      "                '两年': 33808 -> 33808\n",
      "                '真的': 32041 -> 32041\n",
      "                 '吸': 55360 -> 55360\n",
      "                 '粉': 55486 -> 55486\n",
      "                '不少': 32138 -> 32138\n",
      "                 '，': 31123 -> 31123\n",
      "                '明星': 32943 -> 32943\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '达': 54880 -> 54880\n",
      "                '人的': 31664 -> 31664\n",
      "                '心头': 46565 -> 46565\n",
      "                 '爱': 54799 -> 54799\n",
      "                 '。': 31155 -> 31155\n",
      "                '毕竟': 33051 -> 33051\n",
      "                 '好': 54591 -> 54591\n",
      "                 '穿': 55432 -> 55432\n",
      "                '时尚': 33481 -> 33481\n",
      "                 '，': 31123 -> 31123\n",
      "                 '谁': 55622 -> 55622\n",
      "                '都能': 32904 -> 32904\n",
      "                 '穿': 55432 -> 55432\n",
      "                 '出': 54557 -> 54557\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '长': 54625 -> 54625\n",
      "                 '2': 30943 -> 30943\n",
      "                 '米': 55055 -> 55055\n",
      "               '的效果': 35590 -> 35590\n",
      "                '宽松': 40833 -> 40833\n",
      "                 '的': 54530 -> 54530\n",
      "                 '裤': 56532 -> 56532\n",
      "                 '腿': 56158 -> 56158\n",
      "                 '，': 31123 -> 31123\n",
      "               '当然是': 48466 -> 48466\n",
      "                 '遮': 57148 -> 57148\n",
      "                 '肉': 55343 -> 55343\n",
      "                 '小': 54603 -> 54603\n",
      "                '能手': 49355 -> 49355\n",
      "                 '啊': 55674 -> 55674\n",
      "                 '。': 31155 -> 31155\n",
      "                '上身': 51605 -> 51605\n",
      "                 '随': 55119 -> 55119\n",
      "                 '性': 54642 -> 54642\n",
      "                '自然': 31799 -> 31799\n",
      "                 '不': 54535 -> 54535\n",
      "                 '拘': 57036 -> 57036\n",
      "                 '束': 55625 -> 55625\n",
      "                 '，': 31123 -> 31123\n",
      "                '面料': 46839 -> 46839\n",
      "                 '亲': 55113 -> 55113\n",
      "                 '肤': 56089 -> 56089\n",
      "                '舒适': 33894 -> 33894\n",
      "                 '贴': 55778 -> 55778\n",
      "                '身体': 31902 -> 31902\n",
      "                 '验': 55017 -> 55017\n",
      "                 '感': 54706 -> 54706\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '棒': 56382 -> 56382\n",
      "                 '哒': 59230 -> 59230\n",
      "                 '。': 31155 -> 31155\n",
      "                 '系': 54712 -> 54712\n",
      "                 '带': 54882 -> 54882\n",
      "                '部分': 31726 -> 31726\n",
      "                '增加': 31917 -> 31917\n",
      "                '设计': 31735 -> 31735\n",
      "                '看点': 45032 -> 45032\n",
      "                 '，': 31123 -> 31123\n",
      "                 '还': 54656 -> 54656\n",
      "                 '让': 54772 -> 54772\n",
      "                '单品': 46539 -> 46539\n",
      "               '的设计': 34481 -> 34481\n",
      "                 '感': 54706 -> 54706\n",
      "                '更强': 43084 -> 43084\n",
      "                 '。': 31155 -> 31155\n",
      "                '腿部': 46799 -> 46799\n",
      "                '线条': 37216 -> 37216\n",
      "                 '若': 55351 -> 55351\n",
      "                 '隐': 55733 -> 55733\n",
      "                 '若': 55351 -> 55351\n",
      "                 '现': 54600 -> 54600\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                '性感': 40589 -> 40589\n",
      "                 '撩': 58521 -> 58521\n",
      "                 '人': 54533 -> 54533\n",
      "                 '。': 31155 -> 31155\n",
      "                '颜色': 33692 -> 33692\n",
      "                 '敲': 57004 -> 57004\n",
      "                '温柔': 34678 -> 34678\n",
      "                 '的': 54530 -> 54530\n",
      "                 '，': 31123 -> 31123\n",
      "                 '与': 54619 -> 54619\n",
      "                '裤子': 44722 -> 44722\n",
      "                '本身': 32754 -> 32754\n",
      "                 '所': 54626 -> 54626\n",
      "                '呈现': 33169 -> 33169\n",
      "               '的风格': 48084 -> 48084\n",
      "                '有点': 33149 -> 33149\n",
      "                 '反': 54955 -> 54955\n",
      "                 '差': 55342 -> 55342\n",
      "                 '萌': 56842 -> 56842\n",
      "                 '。': 31155 -> 31155\n",
      "                  '': 2 -> 2\n",
      "/root/miniconda3/lib/python3.10/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "从断点继续训练\n",
      "断点文件路径是：['runs', 'checkpoint-2000']\n",
      "resume checkpoint from  checkpoint-2000\n",
      "Loading model from ./output/checkpoint-2000.\n",
      "***** Running training *****\n",
      "  Num examples = 114,599\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3,000\n",
      "  Number of trainable parameters = 1,949,696\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 0\n",
      "  Continuing training from global step 2000\n",
      "  Will skip the first 0 epochs then the first 2000 batches in the first epoch.\n",
      "{'loss': 3.3902, 'grad_norm': 8.904178619384766, 'learning_rate': 1.65e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4963, 'grad_norm': 7.744819164276123, 'learning_rate': 1.6333333333333335e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5561, 'grad_norm': 8.894948959350586, 'learning_rate': 1.6166666666666665e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4924, 'grad_norm': 8.574150085449219, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3707, 'grad_norm': 8.47715950012207, 'learning_rate': 1.5833333333333333e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3279, 'grad_norm': 7.882842540740967, 'learning_rate': 1.5666666666666667e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4424, 'grad_norm': 8.379859924316406, 'learning_rate': 1.55e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4152, 'grad_norm': 8.374821662902832, 'learning_rate': 1.5333333333333334e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4404, 'grad_norm': 7.543618202209473, 'learning_rate': 1.5166666666666668e-05, 'epoch': 0.07}\n",
      "{'loss': 3.359, 'grad_norm': 7.830572128295898, 'learning_rate': 1.5e-05, 'epoch': 0.07}\n",
      "{'loss': 3.2932, 'grad_norm': 7.819767475128174, 'learning_rate': 1.4833333333333336e-05, 'epoch': 0.07}\n",
      "{'loss': 3.5812, 'grad_norm': 8.284809112548828, 'learning_rate': 1.4666666666666668e-05, 'epoch': 0.07}\n",
      "{'loss': 3.2543, 'grad_norm': 7.870508193969727, 'learning_rate': 1.45e-05, 'epoch': 0.07}\n",
      "{'loss': 3.3627, 'grad_norm': 8.36943531036377, 'learning_rate': 1.4333333333333334e-05, 'epoch': 0.07}\n",
      "{'loss': 3.4002, 'grad_norm': 7.5029730796813965, 'learning_rate': 1.4166666666666668e-05, 'epoch': 0.08}\n",
      "{'loss': 3.5152, 'grad_norm': 8.265189170837402, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3977, 'grad_norm': 7.024610996246338, 'learning_rate': 1.3833333333333334e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4127, 'grad_norm': 7.955397605895996, 'learning_rate': 1.3666666666666666e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3533, 'grad_norm': 7.778101444244385, 'learning_rate': 1.3500000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 3.432, 'grad_norm': 7.651393413543701, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4506, 'grad_norm': 7.023166656494141, 'learning_rate': 1.3166666666666665e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4195, 'grad_norm': 7.940942764282227, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.08}\n",
      "{'loss': 3.4195, 'grad_norm': 8.271703720092773, 'learning_rate': 1.2833333333333333e-05, 'epoch': 0.08}\n",
      "{'loss': 3.3715, 'grad_norm': 8.468618392944336, 'learning_rate': 1.2666666666666668e-05, 'epoch': 0.08}\n",
      "{'loss': 3.2398, 'grad_norm': 8.478156089782715, 'learning_rate': 1.25e-05, 'epoch': 0.08}\n",
      " 75%|█████████████████████████████▎         | 2250/3000 [02:20<07:04,  1.77it/s]\n",
      "\n",
      "Training stopped early as loss 3.2398 is below threshold 3.25.\n",
      " 75%|█████████████████████████████▎         | 2251/3000 [02:21<07:45,  1.61it/s]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 141.7445, 'train_samples_per_second': 84.659, 'train_steps_per_second': 21.165, 'train_loss': 0.38012272323411816, 'epoch': 0.08}\n",
      " 75%|█████████████████████████████▎         | 2251/3000 [02:21<07:45,  1.61it/s]\n",
      "\n",
      "模型训练结束，进行存储\n",
      " 75%|█████████████████████████████▎         | 2251/3000 [02:21<00:47, 15.88it/s]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1070\n",
      "  Batch size = 16\n",
      "100%|███████████████████████████████████████████| 67/67 [10:09<00:00,  8.96s/it]Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.578 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "100%|███████████████████████████████████████████| 67/67 [10:14<00:00,  9.18s/it]\n",
      "Saving model checkpoint to ./output_model\n",
      "/root/miniconda3/lib/python3.10/site-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in ../autodl-tmp/model/chatglm3 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#加入早停逻辑，在上面的微调进展下继续微调，并在finetune_hf.py加入限制条件，以让loss结果回归到3.25以下\n",
    "# 更准确的说是到达3.25以下则停止训练\n",
    "\n",
    "!python finetune_hf.py  data/AdvertiseGen_fix  ../autodl-tmp/model/chatglm3  lora.yaml yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28324f28-ec69-47aa-89c6-f9f59ec4c5d5",
   "metadata": {},
   "source": [
    "### 4. 使用微调的数据集进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a52bf95e-57b5-4cea-850b-e8e9caa78139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:03<00:00,  2.19it/s]\n",
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "prompt=类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则\n",
      "等待回复...\n",
      "这款连衣裙的版型很显瘦，压褶的木耳边的设计，很甜美，加上网纱拼接，很性感。网纱的百褶裙摆，很飘逸。套头拉链的设计，很方便，拉链的压褶设计，很别致，很时尚。\n"
     ]
    }
   ],
   "source": [
    "# !python inference_hf.py output/checkpoint-2000/ --prompt \"类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则\"\n",
    "\n",
    "!python inference_hf.py output_model/ --prompt \"类型#裙*版型#显瘦*材质#网纱*风格#性感*裙型#百褶*裙下摆#压褶*裙长#连衣裙*裙衣门襟#拉链*裙衣门襟#套头*裙款式#拼接*裙款式#拉链*裙款式#木耳边*裙款式#抽褶*裙款式#不规则\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a159eef4-381b-4fdf-9b4b-4cb17a29b2a5",
   "metadata": {},
   "source": [
    "### 5. 将微调后的模型部署到 basic_demo下的 gradio_demo 中，并能够通过 webui 来进行调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bf66876-b2a6-4ce3-94db-db50fd1c6b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:03<00:00,  2.24it/s]\n",
      "Setting eos_token is not supported, use the default one.\n",
      "Setting pad_token is not supported, use the default one.\n",
      "Setting unk_token is not supported, use the default one.\n",
      "Running on local URL:  http://127.0.0.1:6006\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n",
      "^C\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "# web_demo_gradio.py已调整好\n",
    "!python web_demo_gradio.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b451f673-7f26-4eaf-bbaa-c771a3c20c45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
